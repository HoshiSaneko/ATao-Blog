---
title: AI大模型常见术语
date: '2025年08月26日'
tags: ['经验分享']
summary: 汇总了AI大模型中常见的名词术语及解释。
---
### 模型基础与结构

#### 大语言模型（LLM, Large Language Model）
基于海量文本训练的大规模语言模型，能理解和生成类人文本（如 GPT、LLaMA、Qwen）

#### Transformer
LLM 的核心架构，基于自注意力机制，支持并行处理序列数据。

#### 自注意力机制
Transformer 的核心，让模型计算文本中每个 Token 与其他 Token 的关联（如 “他” 指代前文的 “小明”）

#### 模型深度
Transformer 的层数（如 GPT-3 有 96 层），深度越深，模型捕捉复杂规律的能力越强。

### 参数与规模

#### 模型参数
模型训练中学习的权重和偏置（如 “7B 模型” 指 70 亿参数），参数量影响模型能力和资源需求。

#### 参数量级
模型参数的规模等级（如 B = 十亿，13B 即 130 亿）。

#### 上下文窗口
模型能处理的最大输入长度（以 Token 为单位），即 “上下文长度”。

#### Token
文本的基本处理单位（如中文单字、英文子词），模型输入输出均以 Token 为单位。

#### 词汇表
模型可识别的所有 Token 集合（如 GPT-2 词汇表约 5 万 Token），未在表中的词会被拆分为子词。

### 训练与优化

#### 预训练
模型在海量通用文本（如书籍、网页）上的初始训练，学习语言规律和世界知识。

#### 预训练数据
用于预训练的文本集合（如 GPT-3 用了约 45TB 文本），数据质量和多样性影响模型能力。

#### 微调
预训练模型在特定任务数据（如医疗问答）上的二次训练，适配具体场景。

#### 指令微调
用 “指令 - 响应” 格式数据微调模型，让模型理解人类指令（如 “写一篇总结”）。

#### 过拟合
模型过度拟合训练数据，在新数据上表现差（如生成内容局限于训练文本）。

#### 泛化能力
模型对未见过的新文本的适应能力，是评估 LLM 的核心指标。

### 数据表示与转换

#### 向量化
将文本、图像等非结构化数据转换为数值向量（Embedding）的过程。

#### 嵌入向量
向量化的结果，是文本语义的数值表示。

#### 向量数据库
专门存储和检索嵌入向量的数据库，通过高效的近似最近邻（ANN）算法，快速从海量向量中找到与查询向量最相似的结果，是 RAG 系统的 “知识库存储核心”。

### 模型压缩与优化

#### 量化
降低模型参数精度（如 FP16→INT8→INT4），减少显存占用和计算量（如 7B 模型从 14GB 降至 3.5GB）。

#### AWQ
一种高效量化方法，通过激活感知权重量化，在低精度下保留模型性能，适合 LLM 部署。

#### GPTQ
另一种常用量化方法，通过优化量化误差，支持 4bit 量化，广泛用于社区模型。

#### 模型蒸馏
一种模型压缩技术：用性能强的 “教师模型”（如 100B 大模型）指导 “学生模型”（如 7B 小模型）学习，通过模仿教师模型的输出分布（而非仅学习标签），让小模型在体积更小、速度更快的同时，保留接近大模型的能力。

#### 知识蒸馏
将教师模型学到的 “暗知识”（如对不同答案的概率分布）迁移给学生模型，而非仅传递 “明知识”（如正确答案），使小模型更懂 “推理逻辑”。

#### 剪枝
直接删除模型中冗余的参数或结构（如权重接近 0 的神经元、贡献度低的网络层），减少模型体积和计算量。与蒸馏不同，剪枝是 “减法”（删参数），蒸馏是 “模仿学习”。

### 数值精度与计算

#### FP32
32 位精度的浮点数，是传统深度学习的默认数据类型，精度最高但显存占用大（1 个 FP32 参数占 4 字节）。大模型训练初期可能用 FP32，但推理时极少使用（太耗资源）。

#### FP16
16 位精度的浮点数，显存占用仅为 FP32 的一半（1 个参数占 2 字节），计算速度更快，广泛用于模型推理和部分训练场景（如混合精度训练）。但精度较低，极端值可能溢出。

#### BF16
16 位精度的浮点数，由谷歌为 TPU 设计，指数位与 FP32 相同（保留大范围数值），尾数位更短（牺牲部分精度）。适合大模型训练（抗噪性强），与 FP16 相比，在极端值处理上更稳定，常见于 A100、H100 等高端 GPU。

#### INT8/INT4
整数精度数据类型，INT8 参数占 1 字节（为 FP32 的 1/4），INT4 仅占 0.5 字节（为 FP32 的 1/8），是量化技术的核心。通过降低精度大幅减少显存占用，适合低资源设备部署。

#### 混合精度
训练 / 推理中同时使用多种精度，平衡精度与效率。

### 输入与输出处理

#### 分词器
将文本拆分为 Token 的工具，是 LLM 处理文本的第一步。

#### Prompt（提示词）
用户输入的指令或问题（如 “解释什么是 LLM”），用于引导模型生成输出。

#### 提示词工程
设计优化 Prompt 的方法（如添加示例、限定格式），提升模型输出质量。

#### 少样本提示
在 Prompt 中加入少量示例（如 “例 1：... 例 2：... 请解决：...”），帮助模型理解任务。

#### 零样本提示
不提供示例，直接用指令让模型处理任务（如 “翻译‘你好’为英文”）。

#### 输出长度
模型生成文本的最大 Token 数，可通过参数限制。

#### 温度参数
控制输出随机性的参数（0 = 确定性输出，1 = 高随机性），值越低输出越固定。

### 部署

#### vLLM
高效的 LLM 推理框架，通过 PagedAttention 技术优化显存使用，提升吞吐量。

#### Tensor Parallelism（张量并行）
将模型参数拆分到多个 GPU 上，解决单卡显存不足问题（如 13B 模型拆到 2 张卡）。

#### Pipeline Parallelism（流水线并行）
将模型层拆分到多个 GPU，按顺序处理数据，提升大模型训练 / 推理效率。

#### KV 缓存（KV Cache）
缓存推理过程中的 Key 和 Value 向量（自注意力计算结果），避免重复计算，加速生成。

### 应用与扩展

#### 生成式 AI
能生成新内容的 AI，LLM 是其核心（如生成文章、代码、对话）。

#### 文本生成
LLM 的核心能力，生成符合语境的文本（如写邮件、编故事）。

#### 问答系统
LLM 根据问题生成答案（如 “地球半径是多少？”→输出具体数值）。

#### 机器翻译
将一种语言转换为另一种（如 LLM 支持中英互译）。

#### 摘要生成
将长文本压缩为简洁摘要（如把一篇论文缩为一段话）。

#### 代码生成
LLM 根据指令生成代码（如 “写一个 Python 排序函数”）。

#### RAG（检索增强生成）
结合外部知识库检索与 LLM 生成，让输出引用真实信息（如企业知识库问答）。

#### 多轮对话
LLM 记住对话历史，支持连续交互（如聊天机器人）。

#### 工具调用
LLM 调用外部工具（如计算器、搜索引擎）完成任务（如 “查今天的天气”）。

#### 多模态
LLM 结合图像、音频等数据（如 GPT-4V 能理解图片 + 文本）。

### 模型类型与特性

#### 基座模型
仅经过预训练的模型，需微调后才能用于具体任务。

#### 对话模型
经指令微调 + RLHF 优化的模型，适合直接对话。

#### 开源模型
公开模型权重和代码，允许自由使用和修改（如 LLaMA2、Qwen）。

#### 闭源模型
不公开权重，仅通过 API 提供服务（如 GPT-4、Claude）。

#### MoE（混合专家模型）
模型包含多个 “专家子网络”，输入由路由层分配给部分专家处理，平衡规模与效率（如 GLaM）。

#### 长上下文模型
支持超长输入的 LLM（如 Qwen3 支持 128k Token，可处理整本书）。

### 评估与问题

#### 困惑度（PPL）
评估模型对文本的预测能力，值越低表示模型越 “理解” 文本（如 PPL=10 优于 PPL=20）。

#### BLEU 分数
评估机器翻译质量的指标，衡量输出与参考译文的重叠度。

#### ROUGE 分数
评估文本摘要质量的指标，对比摘要与原文的相似度。

#### 幻觉
LLM 生成错误或不存在的信息（如编造事实、引用假数据）。

#### 对齐
让 LLM 输出符合人类价值观和需求（如避免有害内容、遵循伦理）。

#### 偏见
LLM 因训练数据包含偏见（如性别、种族偏见），输出带有倾向性内容。

#### 鲁棒性
LLM 对输入微小变化（如错别字）的稳定性，鲁棒性差易输出错误。

### 工具与生态

#### Hugging Face
开源 AI 社区，提供 LLM 模型、工具库（如 Transformers）和部署平台。

#### Transformers 库
Hugging Face 推出的工具库，简化 LLM 加载、训练和推理（支持多框架）。

#### LangChain
构建 LLM 应用的框架，支持 RAG、工具调用、多轮对话等复杂流程。
